{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWdUsSqiE5si",
        "outputId": "f30abf42-3703-4f29-daf6-2a993b6fa148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install torchmetrics\n",
        "import transformers as T\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dH-K7jfxE5sk"
      },
      "outputs": [],
      "source": [
        "# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點\n",
        "token_replacement = [\n",
        "    [\"：\" , \":\"],\n",
        "    [\"，\" , \",\"],\n",
        "    [\"“\" , \"\\\"\"],\n",
        "    [\"”\" , \"\\\"\"],\n",
        "    [\"？\" , \"?\"],\n",
        "    [\"……\" , \"...\"],\n",
        "    [\"！\" , \"!\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0ehE13hHE5sl"
      },
      "outputs": [],
      "source": [
        "model = MultiLabelModel().to(device)\n",
        "tokenizer = T.BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RtjiS3nEE5sl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "#改自己下載的資料集\n",
        "# 創建目標目錄\n",
        "os.makedirs('./cache/sick_data', exist_ok=True)\n",
        "\n",
        "# 文件下載鏈接\n",
        "urls = [\n",
        "    \"https://alt.qcri.org/semeval2014/task1/data/uploads/sick_train.zip\",\n",
        "    \"https://alt.qcri.org/semeval2014/task1/data/uploads/sick_test_annotated.zip\",\n",
        "    \"https://alt.qcri.org/semeval2014/task1/data/uploads/sick_trial.zip\"\n",
        "]\n",
        "\n",
        "# 下載和解壓縮文件\n",
        "for url in urls:\n",
        "    local_filename = os.path.join('./cache/sick_data', url.split('/')[-1])\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        with open(local_filename, 'wb') as f:#wb是寫入二進制文件\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    with zipfile.ZipFile(local_filename, 'r') as zip_ref: #解壓縮\n",
        "        zip_ref.extractall('./cache/sick_data')#解壓縮到指定目錄\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe0240lSE5sm",
        "outputId": "b9a88ba2-22c0-4d99-f263-50580589da99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset example: \n",
            "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
            "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.2, 'entailment_judgment': 0} \n",
            "{'sentence_pair_id': 3, 'premise': 'The young boys the playing outdoors 有時 and are man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.7, 'entailment_judgment': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 同義詞替換函數\n",
        "def synonym_replacement(text, num_replacements=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    replaced_indices = set()  # 記錄已經替換過的單詞位置\n",
        "\n",
        "    for _ in range(num_replacements):\n",
        "        word_to_replace = random.choice(words)\n",
        "        if word_to_replace in replaced_indices:\n",
        "            continue  # 避免重複替換\n",
        "        synonyms = wordnet.synsets(word_to_replace)\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
        "            new_words = [synonym if word == word_to_replace else word for word in words]\n",
        "            replaced_indices.add(word_to_replace)\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 隨機刪除某些單詞\n",
        "def random_deletion(text, p=0.3):\n",
        "    words = text.split()\n",
        "    if len(words) == 1:  # 若只有一個詞，返回原句\n",
        "        return text\n",
        "\n",
        "    # 選擇要刪除的單詞數量\n",
        "    remaining_words = [word for word in words if random.random() > p]\n",
        "\n",
        "    # 如果句子沒有變長，隨機刪除一個單詞\n",
        "    if len(remaining_words) == 0:\n",
        "        remaining_words = random.sample(words, 1)\n",
        "\n",
        "    return ' '.join(remaining_words)\n",
        "\n",
        "# 隨機插入新詞\n",
        "def random_insertion(text, p=0.2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    num_insertions = int(len(words) * p)\n",
        "    for _ in range(num_insertions):\n",
        "        insert_word = random.choice([\"非常\", \"極度\", \"總是\", \"有時\", \"還是\"])\n",
        "        insert_position = random.randint(0, len(new_words))\n",
        "        new_words.insert(insert_position, insert_word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 句子順序調換\n",
        "def random_swap(text, n=3):\n",
        "    words = text.split()\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# 拼寫錯誤增強\n",
        "def random_typo(text, p=0.1):\n",
        "    chars = list(text)\n",
        "    for i in range(len(chars)):\n",
        "        if random.random() < p:\n",
        "            chars[i] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "    return ''.join(chars)\n",
        "\n",
        "# 增強 `class 1` 的文本數據\n",
        "def augment_class1_data(data):\n",
        "    augmented_data = []\n",
        "    for item in data:\n",
        "        if item['entailment_judgment'] == 1:  # 只針對 `class 1` 的數據進行輕微增強\n",
        "            premise_augmented = random_swap(item['premise'], n=1)  # 進行少量的隨機調換\n",
        "            premise_augmented = random_insertion(premise_augmented, p=0.1)  # 插入少量的詞語\n",
        "            hypothesis_augmented = item['hypothesis']  # 保持 hypothesis 不變\n",
        "            augmented_data.append({\n",
        "                'sentence_pair_id': item['sentence_pair_id'],\n",
        "                'premise': premise_augmented,\n",
        "                'hypothesis': hypothesis_augmented,\n",
        "                'relatedness_score': item['relatedness_score'],\n",
        "                'entailment_judgment': item['entailment_judgment']\n",
        "            })\n",
        "        else:\n",
        "            augmented_data.append(item)  # 保持其他類別不變\n",
        "    return augmented_data\n",
        "\n",
        "# 增強 `class 2` 的文本數據\n",
        "def augment_class2_data(data):\n",
        "    augmented_data = []\n",
        "    for item in data:\n",
        "        if item['entailment_judgment'] == 2:  # 只針對 `class 2` 的數據進行強增強\n",
        "            premise_augmented = synonym_replacement(item['premise'], num_replacements=3)  # 同義詞替換\n",
        "            premise_augmented = random_swap(premise_augmented, n=2)  # 進行隨機調換\n",
        "            premise_augmented = random_insertion(premise_augmented, p=0.2)  # 插入詞語\n",
        "            hypothesis_augmented = random_deletion(item['hypothesis'], p=0.2)  # 隨機刪除詞語\n",
        "            hypothesis_augmented = random_typo(hypothesis_augmented, p=0.1)  # 拼寫錯誤\n",
        "            augmented_data.append({\n",
        "                'sentence_pair_id': item['sentence_pair_id'],\n",
        "                'premise': premise_augmented,\n",
        "                'hypothesis': hypothesis_augmented,\n",
        "                'relatedness_score': item['relatedness_score'],\n",
        "                'entailment_judgment': item['entailment_judgment']\n",
        "            })\n",
        "        else:\n",
        "            augmented_data.append(item)  # 保持其他類別不變\n",
        "    return augmented_data\n",
        "\n",
        "# 更新 `SemevalDataset` 類，使用不同的增強方法\n",
        "class SemevalDataset(Dataset):\n",
        "    def __init__(self, split=\"train\") -> None:\n",
        "        super().__init__()\n",
        "        assert split in [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "        # Mapping from split to file\n",
        "        split_to_file = {\n",
        "            \"train\": \"SICK_train.txt\",\n",
        "            \"test\": \"SICK_test_annotated.txt\",\n",
        "            \"validation\": \"SICK_trial.txt\"\n",
        "        }\n",
        "        # Load data from my file\n",
        "        file_path = f'./cache/sick_data/{split_to_file[split]}'\n",
        "        self.data = self.load_local_dataset(file_path)\n",
        "\n",
        "        # 增強數據，先對 class 1 進行輕微增強，再對 class 2 進行強增強\n",
        "        self.data = augment_class1_data(self.data)\n",
        "        self.data = augment_class2_data(self.data)\n",
        "\n",
        "    def load_local_dataset(self, file_path):\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f.readlines()[1:]:  # 跳過標題行\n",
        "                parts = line.strip().split('\\t')\n",
        "                # Mapping for entailment labels\n",
        "                entailment_mapping = {\n",
        "                    \"NEUTRAL\": 0,\n",
        "                    \"ENTAILMENT\": 1,\n",
        "                    \"CONTRADICTION\": 2\n",
        "                }\n",
        "                # Append data to list\n",
        "                data.append({\n",
        "                    \"sentence_pair_id\": int(parts[0]),\n",
        "                    \"premise\": parts[1],\n",
        "                    \"hypothesis\": parts[2],\n",
        "                    \"relatedness_score\": float(parts[3]),\n",
        "                    \"entailment_judgment\": entailment_mapping[parts[4]] # Store numerical label\n",
        "                })\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        d = self.data[index]\n",
        "        # 把中文標點替換掉\n",
        "        for k in [\"premise\", \"hypothesis\"]:\n",
        "            for tok in token_replacement:\n",
        "                d[k] = d[k].replace(tok[0], tok[1])\n",
        "        return d\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# 測試數據增強效果\n",
        "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
        "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6qRuyFIVE5sm"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "lr = 3e-5\n",
        "epochs = 3\n",
        "train_batch_size = 8\n",
        "validation_batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AXgfgTOvE5sm"
      },
      "outputs": [],
      "source": [
        "# TODO1: Create batched data for DataLoader\n",
        "# `collate_fn` is a function that defines how the data batch should be packed.\n",
        "# This function will be called in the DataLoader to pack the data batch.\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # TODO1-1: Implement the collate_fn function\n",
        "    # 使用 tokenizer 將批次的句子進行編碼\n",
        "    premises = [item[\"premise\"] for item in batch]\n",
        "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
        "    relatedness_scores = [item[\"relatedness_score\"] for item in batch]\n",
        "    entailment_judgments = [item[\"entailment_judgment\"] for item in batch]\n",
        "\n",
        "    # 將前提和假設進行編碼，並返回所需的 tensor 格式\n",
        "    encoding = tokenizer(\n",
        "        premises,\n",
        "        hypotheses,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 將標籤轉換為 PyTorch 張量\n",
        "    encoding[\"relatedness_score\"] = torch.tensor(relatedness_scores, dtype=torch.float32)\n",
        "    encoding[\"entailment_judgment\"] = torch.tensor(entailment_judgments, dtype=torch.long)\n",
        "\n",
        "    return encoding\n",
        "\n",
        "# TODO1-2: Define your DataLoader\n",
        "# 定義 DataLoader\n",
        "dl_train = DataLoader(\n",
        "    SemevalDataset(split=\"train\"),\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=False,#打亂數據，因為是訓練集\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "dl_validation = DataLoader(\n",
        "    SemevalDataset(split=\"validation\"),\n",
        "    batch_size=validation_batch_size,\n",
        "    shuffle=False,#不打亂數據，因為是驗證集\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "# Define DataLoader for test set\n",
        "dl_test = DataLoader(\n",
        "    SemevalDataset(split=\"test\"),  # Assuming you have a 'test' split in your dataset\n",
        "    batch_size=validation_batch_size,  # You can adjust the batch size as needed\n",
        "    shuffle=False,  # No need to shuffle for testing\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CDP2hfHEE5sn"
      },
      "outputs": [],
      "source": [
        "# TODO2: Construct your model\n",
        "class MultiLabelModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiLabelModel, self).__init__()\n",
        "        self.bert = T.BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
        "        self.dropout = torch.nn.Dropout(0.3)#dropout層<避免過擬合>\n",
        "        self.fc_relatedness = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.fc_entailment = torch.nn.Linear(self.bert.config.hidden_size, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)#輸入到bert模型\n",
        "        pooled_output = outputs[1]#取得pooling的輸出\n",
        "        pooled_output = self.dropout(pooled_output)#dropout層\n",
        "\n",
        "        relatedness_score = self.fc_relatedness(pooled_output).squeeze(-1)#將維度為1的維度去掉\n",
        "        entailment_judgment = self.fc_entailment(pooled_output)#將pooling的輸出輸入到全連接層\n",
        "\n",
        "        return relatedness_score, entailment_judgment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GaXLA7l6E5sn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3617b0db-d608-4f7f-ce62-c9855369022d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
          ]
        }
      ],
      "source": [
        "# TODO3: Define your optimizer and loss function\n",
        "\n",
        "# TODO3-1: Define your Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)#AdamW優化器\n",
        "\n",
        "# TODO3-2: Define your loss functions (you should have two)\n",
        "loss_fn_relatedness = torch.nn.MSELoss()#均方誤差\n",
        "loss_fn_entailment = torch.nn.CrossEntropyLoss()#交叉熵損失\n",
        "\n",
        "# scoring functions\n",
        "spc = SpearmanCorrCoef()\n",
        "acc = Accuracy(task=\"multiclass\", num_classes=3)\n",
        "f1 = F1Score(task=\"multiclass\", num_classes=3, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v-cUet6E5sn",
        "outputId": "8a496383-b047-4a43-de8a-8a86ecdc5bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [1/3]: 100%|██████████| 563/563 [00:48<00:00, 11.49it/s]\n",
            "Validation epoch [1/3]: 100%|██████████| 63/63 [00:01<00:00, 46.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results - Epoch [1/3]:\n",
            "Spearman Correlation: 0.8542\n",
            "Accuracy: 0.9900\n",
            "F1 Score: 0.9912\n",
            "Confusion Matrix:\n",
            "tensor([[282,   0,   0],\n",
            "        [  5, 139,   0],\n",
            "        [  0,   0,  74]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [2/3]: 100%|██████████| 563/563 [00:48<00:00, 11.50it/s]\n",
            "Validation epoch [2/3]: 100%|██████████| 63/63 [00:01<00:00, 47.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results - Epoch [2/3]:\n",
            "Spearman Correlation: 0.8486\n",
            "Accuracy: 0.9820\n",
            "F1 Score: 0.9842\n",
            "Confusion Matrix:\n",
            "tensor([[279,   3,   0],\n",
            "        [  6, 138,   0],\n",
            "        [  0,   0,  74]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [3/3]: 100%|██████████| 563/563 [00:48<00:00, 11.53it/s]\n",
            "Validation epoch [3/3]: 100%|██████████| 63/63 [00:01<00:00, 47.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results - Epoch [3/3]:\n",
            "Spearman Correlation: 0.8499\n",
            "Accuracy: 0.9780\n",
            "F1 Score: 0.9807\n",
            "Confusion Matrix:\n",
            "tensor([[278,   4,   0],\n",
            "        [  7, 137,   0],\n",
            "        [  0,   0,  74]])\n"
          ]
        }
      ],
      "source": [
        "# 初始化 confusion matrix 指標\n",
        "from torchmetrics import ConfusionMatrix\n",
        "\n",
        "confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=3)\n",
        "for ep in range(epochs):\n",
        "    # 訓練階段\n",
        "    pbar = tqdm(dl_train)  # 創建進度條\n",
        "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")  # 設置進度條描述\n",
        "    model.train()  # 設置模型為訓練模式\n",
        "    total_loss_relatedness = 0.0  # 初始化 relatedness 損失總和\n",
        "    total_loss_entailment = 0.0  # 初始化 entailment 損失總和\n",
        "\n",
        "    for batch in pbar:\n",
        "        # 將數據移動到 GPU 或 CPU 設備\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "        relatedness_score = batch[\"relatedness_score\"].to(device)\n",
        "        entailment_judgment = batch[\"entailment_judgment\"].to(device)\n",
        "\n",
        "        # 清除優化器的梯度\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 前向傳遞：將數據輸入模型，獲取 relatedness 和 entailment 的預測\n",
        "        pred_relatedness, pred_entailment = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # 計算損失：分別計算 relatedness 和 entailment 的損失，並將兩者相加為總損失\n",
        "        loss_relatedness = loss_fn_relatedness(pred_relatedness, relatedness_score)\n",
        "        loss_entailment = loss_fn_entailment(pred_entailment, entailment_judgment)\n",
        "        loss = loss_relatedness + loss_entailment  # 總損失\n",
        "\n",
        "        # 反向傳遞：計算梯度\n",
        "        loss.backward()\n",
        "\n",
        "        # 更新模型參數\n",
        "        optimizer.step()\n",
        "\n",
        "        # 累加損失值，用於後續計算平均損失\n",
        "        total_loss_relatedness += loss_relatedness.item()\n",
        "        total_loss_entailment += loss_entailment.item()\n",
        "\n",
        "    # 驗證階段\n",
        "    pbar = tqdm(dl_validation)  # 創建進度條\n",
        "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")  # 設置進度條描述\n",
        "    model.eval()  # 設置模型為驗證模式（不會更新權重）\n",
        "\n",
        "    # 初始化列表，用於儲存真實值和預測值\n",
        "    val_relatedness_scores = []\n",
        "    val_pred_relatedness = []\n",
        "    val_entailment_judgments = []\n",
        "    val_pred_entailment = []\n",
        "\n",
        "    with torch.no_grad():  # 禁用梯度計算，減少內存消耗\n",
        "        for batch in pbar:\n",
        "            # 將數據移動到設備\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "            relatedness_score = batch[\"relatedness_score\"].to(device)\n",
        "            entailment_judgment = batch[\"entailment_judgment\"].to(device)\n",
        "\n",
        "            # 前向傳遞：獲取模型的預測輸出\n",
        "            pred_relatedness, pred_entailment = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "\n",
        "            # 收集驗證集的真實值和預測值，用於後續的評估\n",
        "            val_relatedness_scores.extend(relatedness_score.cpu().numpy())\n",
        "            val_pred_relatedness.extend(pred_relatedness.cpu().numpy())\n",
        "            val_entailment_judgments.extend(entailment_judgment.cpu().numpy())\n",
        "            val_pred_entailment.extend(torch.argmax(pred_entailment, dim=1).cpu().numpy())\n",
        "\n",
        "    # 計算評估指標\n",
        "    spearman_corr = spc(torch.tensor(val_pred_relatedness), torch.tensor(val_relatedness_scores))  # 計算 Spearman 相關係數\n",
        "    accuracy = acc(torch.tensor(val_pred_entailment), torch.tensor(val_entailment_judgments))  # 計算準確度\n",
        "    f1_score = f1(torch.tensor(val_pred_entailment), torch.tensor(val_entailment_judgments))  # 計算 F1 分數\n",
        "    # 計算混淆矩陣\n",
        "    conf_matrix = confusion_matrix(torch.tensor(val_pred_entailment), torch.tensor(val_entailment_judgments))\n",
        "    # 輸出驗證結果\n",
        "    print(f\"Validation Results - Epoch [{ep+1}/{epochs}]:\")\n",
        "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1_score:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "    # 保存模型的檢查點\n",
        "    os.makedirs(\"./saved_models\", exist_ok=True)\n",
        "    torch.save(model, f'./saved_models/ep{ep}.ckpt')  # 儲存模型檔案至指定路徑\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化混淆矩陣指標\n",
        "from torchmetrics import ConfusionMatrix\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# 假設你有已經訓練過的模型\n",
        "model = torch.load('./saved_models/ep2.ckpt')  # 載入訓練好的模型\n",
        "model.to(device)  # 確保模型移動到正確的設備（GPU/CPU）\n",
        "model.eval()  # 設置模型為評估模式\n",
        "\n",
        "confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=3)\n",
        "\n",
        "# 測試階段\n",
        "pbar = tqdm(dl_test)  # 創建進度條\n",
        "pbar.set_description(f\"Testing\")  # 設置進度條描述\n",
        "\n",
        "# 初始化列表，用於儲存真實值和預測值\n",
        "test_relatedness_scores = []\n",
        "test_pred_relatedness = []\n",
        "test_entailment_judgments = []\n",
        "test_pred_entailment = []\n",
        "\n",
        "# 禁用梯度計算\n",
        "with torch.no_grad():\n",
        "    for batch in pbar:\n",
        "        # 將數據移動到設備\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "        relatedness_score = batch[\"relatedness_score\"].to(device)\n",
        "        entailment_judgment = batch[\"entailment_judgment\"].to(device)\n",
        "\n",
        "        # 前向傳遞：獲取模型的預測輸出\n",
        "        pred_relatedness, pred_entailment = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # 收集測試集的真實值和預測值\n",
        "        test_relatedness_scores.extend(relatedness_score.cpu().numpy())\n",
        "        test_pred_relatedness.extend(pred_relatedness.cpu().numpy())\n",
        "        test_entailment_judgments.extend(entailment_judgment.cpu().numpy())\n",
        "        test_pred_entailment.extend(torch.argmax(pred_entailment, dim=1).cpu().numpy())\n",
        "\n",
        "# 計算評估指標\n",
        "spearman_corr = spc(torch.tensor(test_pred_relatedness), torch.tensor(test_relatedness_scores))  # 計算 Spearman 相關係數\n",
        "accuracy = acc(torch.tensor(test_pred_entailment), torch.tensor(test_entailment_judgments))  # 計算準確度\n",
        "f1_score = f1(torch.tensor(test_pred_entailment), torch.tensor(test_entailment_judgments))  # 計算 F1 分數\n",
        "\n",
        "# 計算混淆矩陣\n",
        "conf_matrix = confusion_matrix(torch.tensor(test_pred_entailment), torch.tensor(test_entailment_judgments))\n",
        "\n",
        "# 輸出測試結果\n",
        "print(f\"Test Results:\")\n",
        "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1_score:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GozdAfTTjk5r",
        "outputId": "2ca92fd8-11e3-41a3-8048-12074c8b4226"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-bdefd6a72980>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('./saved_models/ep2.ckpt')  # 載入訓練好的模型\n",
            "Testing: 100%|██████████| 616/616 [00:12<00:00, 49.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results:\n",
            "Spearman Correlation: 0.8351\n",
            "Accuracy: 0.9874\n",
            "F1 Score: 0.9883\n",
            "Confusion Matrix:\n",
            "tensor([[2779,   14,    0],\n",
            "        [  44, 1370,    0],\n",
            "        [   0,    4,  716]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve42EjnCE5sn"
      },
      "source": [
        "For test set predictions, you can write perform evaluation simlar to #TODO5."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}